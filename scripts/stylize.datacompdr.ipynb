{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zscq5-nDjOB4"
      },
      "source": [
        "## Install The Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKxVnXi47sjj"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets pyarrow requests scrapscii tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UibuLKzpBcv9"
      },
      "outputs": [],
      "source": [
        "!echo 'deb [trusted=yes] https://apt.fury.io/ascii-image-converter/ /' | tee /etc/apt/sources.list.d/ascii-image-converter.list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bllIXG1OBLsQ"
      },
      "outputs": [],
      "source": [
        "!apt update\n",
        "!apt install -y ascii-image-converter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42JO6wlKjVaE"
      },
      "source": [
        "## Load The Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qNm1jYv8NX7"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import io\n",
        "import itertools\n",
        "import json\n",
        "import mimetypes\n",
        "import os\n",
        "import random\n",
        "import subprocess\n",
        "import tempfile\n",
        "import urllib\n",
        "\n",
        "import datasets\n",
        "import huggingface_hub\n",
        "import pyarrow.lib as pl\n",
        "import pyarrow.parquet as pq\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import scrapscii.data\n",
        "import scrapscii.unicode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhbPtBZVjYbZ"
      },
      "source": [
        "## Define The Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6Jq5zmM8c2H"
      },
      "outputs": [],
      "source": [
        "# CONSTANTS ####################################################################\n",
        "\n",
        "TIME_MAX = 0.2\n",
        "\n",
        "WIDTH_MIN = 64\n",
        "WIDTH_MAX = 64 # 128\n",
        "\n",
        "TABLE_LEN = 2**12\n",
        "SHARD_LEN = 2**18\n",
        "TOTAL_LEN = 2**24\n",
        "\n",
        "SKIPS_LEN = 0 # 1422186\n",
        "TABLE_IDX = 0 # 157"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjzFksTz8iCc"
      },
      "outputs": [],
      "source": [
        "# IO ###########################################################################\n",
        "\n",
        "TEMP_PATH = tempfile.mkdtemp()\n",
        "DATA_PATH = '/content/dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq8JRAQKqdAd"
      },
      "outputs": [],
      "source": [
        "# FILTER BY EXT ################################################################\n",
        "\n",
        "EXTENSION_LIST = ['jpeg', 'jpg', 'png', 'bmp', 'webp', 'tiff', 'tif', 'gif']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAbMSr188vry"
      },
      "outputs": [],
      "source": [
        "# SETUP ########################################################################\n",
        "\n",
        "os.makedirs(DATA_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1iE-sTF9cIN"
      },
      "outputs": [],
      "source": [
        "# CHECK ########################################################################\n",
        "\n",
        "CORRUPTED_HASH = ['4dcb57651a75abfd07fb36c70c6c5108c49bdb34']\n",
        "\n",
        "def is_valid_response(response: requests.models.Response) -> bool:\n",
        "    return (\n",
        "        bool(response)\n",
        "        and type(response) == requests.models.Response\n",
        "        and response.status_code == 200)\n",
        "\n",
        "def is_valid_extension(extension: str, accepted: list=EXTENSION_LIST) -> bool:\n",
        "    return (\n",
        "        bool(extension)\n",
        "        and type(extension) == str\n",
        "        and extension.lower().strip('.') in accepted)\n",
        "\n",
        "def is_valid_image(image: bytes) -> bool:\n",
        "    return (\n",
        "        bool(image)\n",
        "        and type(image) == bytes\n",
        "        and not hashlib.sha1(image).hexdigest() in CORRUPTED_HASH)\n",
        "\n",
        "def is_valid_ascii(ascii: str, width: int=WIDTH_MIN) -> bool:\n",
        "    return (\n",
        "        bool(ascii)\n",
        "        and type(ascii) == str\n",
        "        and len(ascii) >= width\n",
        "        and not 'error' in ascii.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMxRtAstbhCx"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD #####################################################################\n",
        "\n",
        "def download_image(url: str, timeout: int=1) -> requests.models.Response:\n",
        "    __response = None\n",
        "    # retrieve the image content as bytes\n",
        "    try:\n",
        "        __response = requests.get(url, timeout=timeout)\n",
        "    # ignore exceptions\n",
        "    except:\n",
        "        __response = None\n",
        "    # default\n",
        "    return __response\n",
        "\n",
        "def parse_content(response: requests.models.Response) -> bytes:\n",
        "    __bytes = b''\n",
        "    if is_valid_response(response):\n",
        "        __bytes = response.content\n",
        "    return __bytes\n",
        "\n",
        "def parse_extension(response: requests.models.Response) -> str:\n",
        "    __extension = ''\n",
        "    # parse the header\n",
        "    if is_valid_response(response):\n",
        "        __headers = response.headers.get('content-type', '')\n",
        "        __extension = mimetypes.guess_extension(__headers)\n",
        "    # parse the URL\n",
        "    if not is_valid_extension(__extension):\n",
        "        __path = urllib.parse.urlparse(response.url).path\n",
        "        __filename = __path.split('/')[-1]\n",
        "        __extension = os.path.splitext(__filename)[-1]\n",
        "    # favor the information coming from the header\n",
        "    return __extension\n",
        "\n",
        "def format_path(url: str, extension: str, temp: str=TEMP_PATH) -> str:\n",
        "    # reduce the filename to a fixed size\n",
        "    __hash = hashlib.sha1(url.encode('utf-8')).hexdigest()\n",
        "    # safe path\n",
        "    return os.path.join(temp, __hash + '.' + extension.strip('.'))\n",
        "\n",
        "def export_image(data: bytes, path: str) -> None:\n",
        "    with open(path, 'b+w') as __file:\n",
        "        __file.write(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3kpYaLPUVcV"
      },
      "outputs": [],
      "source": [
        "# STATS ########################################################################\n",
        "\n",
        "def init_stats(\n",
        "    index: int=0,\n",
        "    saved: int=0,\n",
        "    skipped: int=0,\n",
        "    valid: int=0,\n",
        "    response: int=0,\n",
        "    extension: int=0,\n",
        "    image: int=0,\n",
        "    asciiart: int=0\n",
        ") -> dict:\n",
        "    return {\n",
        "        'index': index,\n",
        "        'total': max(saved, skipped),\n",
        "        'saved': saved,\n",
        "        'skipped': skipped,\n",
        "        'valid': valid,\n",
        "        'invalid': {\n",
        "            'response': response,\n",
        "            'extension': extension,\n",
        "            'image': image,\n",
        "            'asciiart': asciiart,},}\n",
        "\n",
        "def update_stats(\n",
        "    stats: dict,\n",
        "    index: int=0,\n",
        "    saved: int=0,\n",
        "    skipped: int=0,\n",
        "    valid: int=0,\n",
        "    response: int=0,\n",
        "    extension: int=0,\n",
        "    image: int=0,\n",
        "    asciiart: int=0\n",
        ") -> dict:\n",
        "    return {\n",
        "        'index': stats['index'] + index,\n",
        "        'total': stats['total'] + skipped + valid + response + extension + image + asciiart,\n",
        "        'saved': saved or stats['saved'], # keep the latest\n",
        "        'skipped': stats['skipped'] + skipped,\n",
        "        'valid': stats['valid'] + valid,\n",
        "        'invalid': {\n",
        "            'response': stats['invalid']['response'] + response,\n",
        "            'extension': stats['invalid']['extension'] + extension,\n",
        "            'image': stats['invalid']['image'] + image,\n",
        "            'asciiart': stats['invalid']['asciiart'] + asciiart,},}\n",
        "\n",
        "def format_stats(stats: dict) -> str:\n",
        "    return 'index={index} total={total} saved={saved} skipped={skipped} valid={valid} invalid={invalid} (response={response} extension={extension} image={image} asciiart={asciiart})'.format(\n",
        "        index=stats['index'],\n",
        "        total=stats['total'],\n",
        "        saved=stats['saved'],\n",
        "        skipped=stats['skipped'],\n",
        "        valid=stats['valid'],\n",
        "        invalid=sum([__v for __v in stats['invalid'].values()]),\n",
        "        response=stats['invalid']['response'],\n",
        "        extension=stats['invalid']['extension'],\n",
        "        image=stats['invalid']['image'],\n",
        "        asciiart=stats['invalid']['asciiart'],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCqh1TMZbjtG"
      },
      "outputs": [],
      "source": [
        "# CLEAR ########################################################################\n",
        "\n",
        "def list_files(path: str, extension: str='') -> list:\n",
        "    return [\n",
        "        os.path.join(__dp, __f)\n",
        "        for __dp, __dn, __fn in os.walk(path)\n",
        "        for __f in __fn if extension in __f]\n",
        "\n",
        "def clear_dir(path: str) -> None:\n",
        "    __paths = list_files(path)\n",
        "    for __p in __paths:\n",
        "        os.remove(__p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FXXrcZcbzW0"
      },
      "outputs": [],
      "source": [
        "# RANDOM #######################################################################\n",
        "\n",
        "def random_options(width_min: int=WIDTH_MIN, width_max: int=WIDTH_MAX) -> list:\n",
        "    # choose the config randomly\n",
        "    __width = '--width {width}'.format(width=random.randint(width_min, width_max))\n",
        "    __braille = '--braille' if random.choice([True, False]) else ''\n",
        "    __color = '--color' if random.choice([True] + 9 * [False]) else ''\n",
        "    __complex = '--complex' if random.choice([True, False]) else ''\n",
        "    __dither = '--dither' if __braille and random.choice([True, False]) else ''\n",
        "    __grayscale = '--grayscale' if random.choice([True] + 9 * [False]) else ''\n",
        "    __negative = '--negative' if random.choice([True] + 9 * [False]) else ''\n",
        "    __threshold = '--threshold {threshold}'.format(threshold=random.randint(96, 160)) if __braille and random.choice([True, False]) else ''\n",
        "    # chain all the options\n",
        "    return [__width, __braille, __color, __complex, __dither, __grayscale, __negative, __threshold]\n",
        "\n",
        "def format_args(options: list) -> list:\n",
        "    return list(itertools.chain.from_iterable(__o.split(' ') for __o in options if __o))\n",
        "\n",
        "def format_labels(options: list) -> list:\n",
        "    return [__o.strip('--') for __o in options if __o]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfqLLcSFb_pC"
      },
      "outputs": [],
      "source": [
        "# EXPORT #######################################################################\n",
        "\n",
        "def export_table(table: iter, index: int, path: str=DATA_PATH) -> None:\n",
        "    __path = os.path.join(path, '{index:0>4d}.parquet'.format(index=index))\n",
        "    scrapscii.data.export_table_as_parquet(table=table, path=__path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGx8SiteupDC"
      },
      "outputs": [],
      "source": [
        "# IMAGE ########################################################################\n",
        "\n",
        "def convert_image(path: str, options: list, timeout: int=TIME_MAX) -> str:\n",
        "    __ascii = ''\n",
        "    # run binary tool\n",
        "    try:\n",
        "        __process = subprocess.run(['ascii-image-converter'] + options + [path], stdout=subprocess.PIPE, timeout=timeout)\n",
        "        __ascii = __process.stdout.decode('utf-8')\n",
        "    # timeout longer executions\n",
        "    except:\n",
        "        __ascii = ''\n",
        "    # default\n",
        "    return __ascii"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2BwDXnd9gb8"
      },
      "outputs": [],
      "source": [
        "# PROCESS ######################################################################\n",
        "\n",
        "def convert_shard(\n",
        "    dataset: iter,\n",
        "    table: iter=[],\n",
        "    stats: dict=init_stats(),\n",
        "    table_len: int=TABLE_LEN,\n",
        "    shard_len: int=SHARD_LEN,\n",
        "    width_min: int=WIDTH_MIN,\n",
        "    width_max: int=WIDTH_MAX,\n",
        "    temp_path: str=TEMP_PATH,\n",
        "    data_path: str=DATA_PATH,\n",
        "    time_max: int=TIME_MAX,\n",
        ") -> tuple:\n",
        "    # current table\n",
        "    __table = list(table)\n",
        "\n",
        "    # take a shard's worth of data\n",
        "    __iter = itertools.islice(dataset, 0, shard_len)\n",
        "\n",
        "    # track progress\n",
        "    __pbar = tqdm.tqdm(__iter, total=shard_len, smoothing=0.0)\n",
        "    __stats = dict(stats)\n",
        "\n",
        "    # iterate over the samples\n",
        "    for __sample in __pbar:\n",
        "\n",
        "        # parse the URL\n",
        "        __url = __sample['url.txt']\n",
        "\n",
        "        # download image from URL\n",
        "        __response = download_image(__url, timeout=time_max)\n",
        "        if not is_valid_response(__response):\n",
        "            __stats = update_stats(stats=__stats, response=1)\n",
        "            __pbar.set_postfix_str(format_stats(__stats), refresh=True)\n",
        "            continue\n",
        "\n",
        "        # parse the extension\n",
        "        __extension = parse_extension(__response)\n",
        "        if not is_valid_extension(__extension):\n",
        "            __stats = update_stats(stats=__stats, extension=1)\n",
        "            __pbar.set_postfix_str(format_stats(__stats), refresh=True)\n",
        "            continue\n",
        "\n",
        "        # parse the image content\n",
        "        __bytes = parse_content(__response)\n",
        "        if not is_valid_image(__bytes):\n",
        "            __stats = update_stats(stats=__stats, image=1)\n",
        "            __pbar.set_postfix_str(format_stats(__stats), refresh=True)\n",
        "            continue\n",
        "\n",
        "        # save to disk\n",
        "        __path = format_path(url=__url, extension=__extension, temp=temp_path)\n",
        "        export_image(data=__bytes, path=__path)\n",
        "\n",
        "        # choose the config randomly\n",
        "        __options = random_options(width_min=width_min, width_max=width_max)\n",
        "        __args = format_args(__options)\n",
        "        __labels = format_labels(__options)\n",
        "\n",
        "        # choose a caption among the synthetic text\n",
        "        __choice = random.randint(0, len(__sample['syn.json']['syn_text']) - 1)\n",
        "        __caption = __sample['syn.json']['syn_text'][__choice]\n",
        "\n",
        "        # convert the image to ASCII art\n",
        "        __content = convert_image(path=__path, options=__args, timeout=time_max)\n",
        "        if not is_valid_ascii(__content):\n",
        "            __stats = update_stats(stats=__stats, asciiart=1)\n",
        "            __pbar.set_postfix_str(format_stats(__stats), refresh=True)\n",
        "            continue\n",
        "\n",
        "        # add a row\n",
        "        __stats = update_stats(stats=__stats, valid=1)\n",
        "        __pbar.set_postfix_str(format_stats(__stats), refresh=True)\n",
        "        __table.append({\n",
        "            'caption': __caption,\n",
        "            'content': __content,\n",
        "            'labels': ','.join(__labels),\n",
        "            'charsets': ','.join(set(scrapscii.unicode.lookup_section(__c) for __c in __content)),\n",
        "            'chartypes': ','.join(set(scrapscii.unicode.lookup_category(__c) for __c in __content)),})\n",
        "\n",
        "        # chunk the dataset into shards\n",
        "        if len(__table) >= table_len:\n",
        "            # export as parquet\n",
        "            export_table(table=__table, index=__stats['index'], path=data_path)\n",
        "            # refresh the stats\n",
        "            __stats = update_stats(stats=__stats, index=1, saved=__stats['total'])\n",
        "            __pbar.set_postfix_str(format_stats(__stats), refresh=True)\n",
        "            __pbar.write(f\"{__stats['total']}\")\n",
        "            # clear the table\n",
        "            __table = []\n",
        "\n",
        "    # return the remainder\n",
        "    return (__stats, __table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcqHim8Pjgee"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkHllEaQcW6X"
      },
      "outputs": [],
      "source": [
        "# SETUP ########################################################################\n",
        "\n",
        "# init the stats\n",
        "__stats = init_stats(index=TABLE_IDX)\n",
        "\n",
        "# init the table\n",
        "__table = []\n",
        "\n",
        "# init the dataset\n",
        "__dataset = datasets.load_dataset('apple/DataCompDR-12M', split='train', cache_dir='~/.cache/huggingface/datasets', streaming=True)\n",
        "__iter = itertools.islice(__dataset, 0, TOTAL_LEN)\n",
        "\n",
        "# skip samples that are already processed\n",
        "__skip = tqdm.tqdm(itertools.islice(__iter, 0, SKIPS_LEN), total=SKIPS_LEN, smoothing=0.0)\n",
        "for _ in __skip:\n",
        "    __stats = update_stats(stats=__stats, skipped=1)\n",
        "\n",
        "# update the latest checkpoint\n",
        "__stats = update_stats(stats=__stats, saved=__stats['total'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd40ouJg3WEX"
      },
      "outputs": [],
      "source": [
        "# MAIN #########################################################################\n",
        "\n",
        "# export a shard\n",
        "__stats, __table = convert_shard(\n",
        "    dataset=__iter,\n",
        "    table=__table,\n",
        "    stats=__stats,\n",
        "    table_len=TABLE_LEN,\n",
        "    shard_len=SHARD_LEN,\n",
        "    width_min=WIDTH_MIN,\n",
        "    width_max=WIDTH_MAX,\n",
        "    temp_path=TEMP_PATH,\n",
        "    data_path=DATA_PATH,\n",
        "    time_max=TIME_MAX,)\n",
        "\n",
        "# remove the temp downloads (images)\n",
        "clear_dir(TEMP_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYLzo_j9ww58"
      },
      "outputs": [],
      "source": [
        "__d = pq.ParquetDataset('dataset/')\n",
        "__t = __d.fragments[0].to_table()\n",
        "__i = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp0nhMmFuxW7"
      },
      "outputs": [],
      "source": [
        "__i += 1\n",
        "print(__t['caption'][__i], __t['labels'][__i])\n",
        "print(__t['content'][__i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 7-bit C1 ANSI sequences\n",
        "ansi_escape = re.compile(r'''\n",
        "    \\x1B  # ESC\n",
        "    (?:   # 7-bit C1 Fe (except CSI)\n",
        "        [@-Z\\\\-_]\n",
        "    |     # or [ for CSI, followed by a control sequence\n",
        "        \\[\n",
        "        [0-?]*  # Parameter bytes\n",
        "        [ -/]*  # Intermediate bytes\n",
        "        [@-~]   # Final byte\n",
        "    )\n",
        "''', re.VERBOSE)"
      ],
      "metadata": {
        "id": "6f3yX5lYyWHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ansi_escape.sub('', __t['content'][__i].as_py()))"
      ],
      "metadata": {
        "id": "KiMNbmb-xwCE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}