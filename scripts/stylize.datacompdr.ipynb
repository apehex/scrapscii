{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UibuLKzpBcv9"
      },
      "outputs": [],
      "source": [
        "!echo 'deb [trusted=yes] https://apt.fury.io/ascii-image-converter/ /' | tee /etc/apt/sources.list.d/ascii-image-converter.list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bllIXG1OBLsQ"
      },
      "outputs": [],
      "source": [
        "!apt update\n",
        "!apt install -y ascii-image-converter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKxVnXi47sjj"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets pyarrow requests scrapscii tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qNm1jYv8NX7"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import io\n",
        "import itertools\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import subprocess\n",
        "import tempfile\n",
        "import urllib\n",
        "\n",
        "import datasets\n",
        "import pyarrow.lib as pl\n",
        "import pyarrow.parquet as pq\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import scrapscii.data\n",
        "import scrapscii.unicode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6Jq5zmM8c2H"
      },
      "outputs": [],
      "source": [
        "# CONSTANTS ####################################################################\n",
        "\n",
        "TIME_MAX = 0.08\n",
        "\n",
        "WIDTH_MIN = 16\n",
        "WIDTH_MAX = 128\n",
        "\n",
        "TABLE_LEN = 2**14\n",
        "SHARD_LEN = 2**18\n",
        "TOTAL_LEN = 2**24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjzFksTz8iCc"
      },
      "outputs": [],
      "source": [
        "# IO ###########################################################################\n",
        "\n",
        "TEMP_PATH = tempfile.mkdtemp()\n",
        "DATA_PATH = '/content/dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAbMSr188vry"
      },
      "outputs": [],
      "source": [
        "# SETUP ########################################################################\n",
        "\n",
        "os.makedirs(DATA_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1iE-sTF9cIN"
      },
      "outputs": [],
      "source": [
        "# CHECK ########################################################################\n",
        "\n",
        "CORRUPTED_HASH = ['4dcb57651a75abfd07fb36c70c6c5108c49bdb34']\n",
        "\n",
        "def is_valid_image(image: bytes) -> bool:\n",
        "    return (\n",
        "        bool(image)\n",
        "        and type(image) == bytes\n",
        "        and not hashlib.sha1(image).hexdigest() in CORRUPTED_HASH)\n",
        "\n",
        "def is_valid_ascii(ascii: str, width: int=WIDTH_MIN) -> bool:\n",
        "    return (\n",
        "        bool(ascii)\n",
        "        and type(ascii) == str\n",
        "        and len(ascii) >= width\n",
        "        and not 'error: can\\'t decode' in ascii.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMxRtAstbhCx"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD #####################################################################\n",
        "\n",
        "def download_image(url: str, timeout: int=1) -> bytes:\n",
        "    __bytes = b''\n",
        "    # retrieve the image content as bytes\n",
        "    try:\n",
        "        __response = requests.get(url, timeout=timeout)\n",
        "        __bytes = __response.content\n",
        "    # ignore exceptions\n",
        "    except:\n",
        "        __bytes = b''\n",
        "    # default\n",
        "    return __bytes\n",
        "\n",
        "def format_path(url: str, temp: str=TEMP_PATH) -> str:\n",
        "    # parse the URL\n",
        "    __path = urllib.parse.urlparse(url).path\n",
        "    __filename = __path.split('/')[-1]\n",
        "    __extension = os.path.splitext(__filename)[-1]\n",
        "    # reduce the filename to a fixed size\n",
        "    __hash = hashlib.sha1(url.encode('utf-8')).hexdigest()\n",
        "    # safe path\n",
        "    return os.path.join(temp, __hash + __extension)\n",
        "\n",
        "def export_image(data: bytes, path: str) -> None:\n",
        "    with open(path, 'b+w') as __file:\n",
        "        __file.write(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCqh1TMZbjtG"
      },
      "outputs": [],
      "source": [
        "# CLEAR ########################################################################\n",
        "\n",
        "def list_files(path: str, extension: str='') -> list:\n",
        "    return [\n",
        "        os.path.join(__dp, __f)\n",
        "        for __dp, __dn, __fn in os.walk(path)\n",
        "        for __f in __fn if extension in __f]\n",
        "\n",
        "def clear_dir(path: str) -> None:\n",
        "    __paths = list_files(path)\n",
        "    for __p in __paths:\n",
        "        os.remove(__p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FXXrcZcbzW0"
      },
      "outputs": [],
      "source": [
        "# RANDOM #######################################################################\n",
        "\n",
        "def random_options(width_min: int=WIDTH_MIN, width_max: int=WIDTH_MAX) -> list:\n",
        "    # choose the config randomly\n",
        "    __width = '--width {width}'.format(width=random.randint(width_min, width_max))\n",
        "    __braille = '--braille' if random.choice([True, False]) else ''\n",
        "    __complex = '--complex' if random.choice([True, False]) else ''\n",
        "    __dither = '--dither' if __braille and random.choice([True, False]) else ''\n",
        "    __grayscale = '--grayscale' if random.choice([False]) else '' # colorless terminal\n",
        "    __negative = '--negative' if random.choice([True, False]) else ''\n",
        "    # chain all the options\n",
        "    return [__width, __braille, __complex, __dither, __grayscale, __negative]\n",
        "\n",
        "def format_args(options: list) -> list:\n",
        "    return list(itertools.chain.from_iterable(__o.split(' ') for __o in options if __o))\n",
        "\n",
        "def format_labels(options: list) -> list:\n",
        "    return list(itertools.chain.from_iterable(__o.strip('--') for __o in options if __o))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfqLLcSFb_pC"
      },
      "outputs": [],
      "source": [
        "# EXPORT #######################################################################\n",
        "\n",
        "def export_table(table: iter, index: int, path: str=DATA_PATH) -> None:\n",
        "    __path = os.path.join(path, '{index:0>4d}.parquet'.format(index=index))\n",
        "    scrapscii.data.export_table_as_parquet(table=table, path=__path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2BwDXnd9gb8"
      },
      "outputs": [],
      "source": [
        "# CONVERT ######################################################################\n",
        "\n",
        "def convert_shard(\n",
        "    dataset: iter,\n",
        "    table: iter=[],\n",
        "    table_idx: int=0,\n",
        "    table_len: int=TABLE_LEN,\n",
        "    shard_len: int=SHARD_LEN,\n",
        "    width_min: int=WIDTH_MIN,\n",
        "    width_max: int=WIDTH_MAX,\n",
        "    temp_path: str=TEMP_PATH,\n",
        "    data_path: str=DATA_PATH,\n",
        "    time_max: int=TIME_MAX,\n",
        ") -> tuple:\n",
        "    # current table\n",
        "    __index = table_idx # index\n",
        "    __table = list(table) # data\n",
        "\n",
        "    # take a shard's worth of data\n",
        "    __iter = itertools.islice(dataset, 0, shard_len)\n",
        "\n",
        "    # track progress\n",
        "    __pbar = tqdm.tqdm(__iter, total=shard_len)\n",
        "    __skip = 0\n",
        "\n",
        "    # iterate over the samples\n",
        "    for __sample in __pbar:\n",
        "\n",
        "        # parse the URL\n",
        "        __url = __sample['url.txt']\n",
        "        __path = format_path(url=__url, temp=temp_path)\n",
        "\n",
        "        # download image from URL\n",
        "        __bytes = download_image(__url, timeout=time_max)\n",
        "\n",
        "        # check hex digest\n",
        "        if is_valid_image(__bytes):\n",
        "            export_image(data=__bytes, path=__path)\n",
        "        else:\n",
        "            __skip += 1\n",
        "            __pbar.set_postfix({'skipped': __skip}, refresh=True)\n",
        "            continue\n",
        "\n",
        "        # choose the config randomly\n",
        "        __options = random_options(width_min=width_min, width_max=width_max)\n",
        "        __args = format_args(__options)\n",
        "        __labels = format_labels(__options)\n",
        "\n",
        "        # choose a caption among the synthetic text\n",
        "        __choice = random.randint(0, len(__sample['syn.json']['syn_text']) - 1)\n",
        "        __caption = __sample['syn.json']['syn_text'][__choice]\n",
        "\n",
        "        # convert the image to ASCII art\n",
        "        try:\n",
        "            __process = subprocess.run(['ascii-image-converter'] + __args + [__path], stdout=subprocess.PIPE, timeout=time_max)\n",
        "            __content = __process.stdout.decode('utf-8')\n",
        "        except:\n",
        "            __skip += 1\n",
        "            __pbar.set_postfix({'skipped': __skip}, refresh=True)\n",
        "            continue\n",
        "\n",
        "        # check for conversion errors\n",
        "        if is_valid_ascii(__content):\n",
        "            __table.append({\n",
        "                'caption': __caption,\n",
        "                'content': __content,\n",
        "                'labels': ','.join(__labels),\n",
        "                'charsets': ','.join(set(scrapscii.unicode.lookup_section(__c) for __c in __content)),\n",
        "                'chartypes': ','.join(set(scrapscii.unicode.lookup_category(__c) for __c in __content)),})\n",
        "        else:\n",
        "            __skip += 1\n",
        "            __pbar.set_postfix({'skipped': __skip}, refresh=True)\n",
        "            continue\n",
        "\n",
        "        # chunk the dataset into shards\n",
        "        if len(__table) >= table_len:\n",
        "            # export as parquet\n",
        "            export_table(table=__table, index=__index, path=data_path)\n",
        "            # refresh\n",
        "            __index += 1\n",
        "            __table = []\n",
        "\n",
        "    # return the remainder\n",
        "    return (__index, __table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkHllEaQcW6X"
      },
      "outputs": [],
      "source": [
        "# MAIN #########################################################################\n",
        "\n",
        "# init the table\n",
        "__index = 0 # index\n",
        "__table = [] # data\n",
        "\n",
        "# init the dataset\n",
        "__dataset = datasets.load_dataset('apple/DataCompDR-12M', split='train', cache_dir='~/.cache/huggingface/datasets', streaming=True)\n",
        "__iter = itertools.islice(__dataset, 0, TOTAL_LEN)\n",
        "\n",
        "# convert shard by shard\n",
        "while __iter:\n",
        "    # export a shard\n",
        "    __index, __table = convert_shard(\n",
        "        dataset=__iter,\n",
        "        table=__table,\n",
        "        table_idx=__index,\n",
        "        table_len=TABLE_LEN,\n",
        "        shard_len=SHARD_LEN,\n",
        "        width_min=WIDTH_MIN,\n",
        "        width_max=WIDTH_MAX,\n",
        "        temp_path=TEMP_PATH,\n",
        "        data_path=DATA_PATH,\n",
        "        time_max=TIME_MAX,)\n",
        "    # remove the temp downloads (images)\n",
        "    clear_dir(TEMP_PATH)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}